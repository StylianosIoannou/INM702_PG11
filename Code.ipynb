{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   Year Age_Group  Gender  Smoking_Prevalence  Drug_Experimentation  \\\n",
      "0  2024     15-19    Both               18.85                 32.40   \n",
      "1  2024     10-14  Female               34.88                 41.57   \n",
      "2  2023     10-14    Both               42.00                 56.80   \n",
      "3  2024     40-49    Both               33.75                 42.90   \n",
      "4  2023     15-19    Male               47.90                 39.62   \n",
      "\n",
      "  Socioeconomic_Status  Peer_Influence School_Programs  Family_Background  \\\n",
      "0                 High               5             Yes                  1   \n",
      "1                 High               6             Yes                 10   \n",
      "2                 High               6             Yes                  2   \n",
      "3               Middle              10              No                  9   \n",
      "4                 High               1              No                  2   \n",
      "\n",
      "   Mental_Health Access_to_Counseling  Parental_Supervision  \\\n",
      "0              5                   No                     4   \n",
      "1              5                   No                     9   \n",
      "2              7                  Yes                     2   \n",
      "3              7                  Yes                     2   \n",
      "4              4                  Yes                     4   \n",
      "\n",
      "  Substance_Education  Community_Support  Media_Influence  \n",
      "0                  No                  3                1  \n",
      "1                 Yes                  9                3  \n",
      "2                  No                  5                1  \n",
      "3                  No                 10                9  \n",
      "4                  No                 10                3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('youth_smoking_drug_data_10000_rows_expanded.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Initial Shape of the Dataset: (10000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Initial Shape of the Dataset:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,x):\n",
    "        # Computed the sigmoind function\n",
    "        # The result is stored in self.output for use during the backward pass\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        return grad * self.output * (1 - self.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        # Save the input for use in the backward pass\n",
    "        self.input = x  \n",
    "        relu_output = np.maximum(0, x)\n",
    "        return np.maximum(0, x)  # Apply ReLU activation\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Create a copy of grad to avoid modifying it directly\n",
    "        grad_input = np.array(grad, copy=True)\n",
    "        # Set the gradient to 0 for all input values where x <= 0\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input  # Return the modified gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        # Normalize by dividing by the sum of exponentials\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Gradient w.r.t. the input is simply grad passed from the loss\n",
    "        # Cross-entropy loss simplifies this calculation\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        \n",
    "        if training:\n",
    "            # Generate the dropout mask (1 for active neurons, 0 for dropped neurons)\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=inputs.shape)\n",
    "            # Apply the mask and scale the active neurons\n",
    "            return inputs * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # At test time, just return the inputs unchanged\n",
    "            return inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "       \n",
    "        # Propagate gradients only through active neurons\n",
    "        return grad_output * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6930474705917861\n",
      "Epoch 10, Loss: 0.6930762532003921\n",
      "Epoch 20, Loss: 0.6930514530592932\n",
      "Epoch 30, Loss: 0.6930475482643893\n",
      "Epoch 40, Loss: 0.6930800190979515\n",
      "Epoch 50, Loss: 0.6930962975076013\n",
      "Epoch 60, Loss: 0.6931142022016307\n",
      "Epoch 70, Loss: 0.6931185556748413\n",
      "Epoch 80, Loss: 0.693078058037064\n",
      "Epoch 90, Loss: 0.6930729429847132\n",
      "Epoch 100, Loss: 0.6930659579025633\n",
      "Epoch 110, Loss: 0.6931427861733764\n",
      "Epoch 120, Loss: 0.6930469609429798\n",
      "Epoch 130, Loss: 0.6930957287355047\n",
      "Epoch 140, Loss: 0.6930759238195155\n",
      "Epoch 150, Loss: 0.6930465273219636\n",
      "Epoch 160, Loss: 0.693054560016322\n",
      "Epoch 170, Loss: 0.6931483466839354\n",
      "Epoch 180, Loss: 0.6930467414987997\n",
      "Epoch 190, Loss: 0.6930891431435964\n",
      "Training Loss: 0.6930580446551555\n",
      "Test Loss: 0.6927936140926708\n",
      "Test Accuracy: 0.514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "class FullyConnectedNN:\n",
    "    #initializing neural netwroks weights, biases and regularization\n",
    "    def __init__(self, layer_sizes, activations, dropout_rates=None, l1=0.01, l2=0.01):\n",
    "        self.layer_sizes = layer_sizes #number of neurons per layer\n",
    "        self.activations = activations #which acitvation function is used for each layer\n",
    "        self.dropout_rates = dropout_rates or [0.0] * (len(layer_sizes) - 1) #dropout rate for each layer\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def _activation_forward(self, x, activation):\n",
    "        #forward pass for activation functions we have coded \n",
    "        if activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'softmax':\n",
    "            exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "    def _activation_backward(self, grad, activation, output):\n",
    "        #backward pass for activation functions we have coded\n",
    "        if activation == 'relu':\n",
    "            grad[output <= 0] = 0\n",
    "            return grad\n",
    "        elif activation == 'sigmoid':\n",
    "            return grad * output * (1 - output)\n",
    "        elif activation == 'softmax':\n",
    "            return grad\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.activations_cache = [X]\n",
    "        self.z_cache = []\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.activations_cache[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_cache.append(z)\n",
    "            a = self._activation_forward(z, self.activations[i])\n",
    "            if training and self.dropout_rates[i] > 0.0:\n",
    "                dropout_mask = np.random.binomial(1, 1 - self.dropout_rates[i], size=a.shape)\n",
    "                a *= dropout_mask / (1 - self.dropout_rates[i])\n",
    "            self.activations_cache.append(a)\n",
    "        return self.activations_cache[-1]\n",
    "\n",
    "    def backward(self, X, y, lr, batch_size):\n",
    "        m = X.shape[0]\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        output_error = self.activations_cache[-1] - y  \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(self.activations_cache[i].T, output_error) / batch_size\n",
    "            grad_b[i] = np.sum(output_error, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "            grad_w[i] += self.l1 * np.sign(self.weights[i]) + self.l2 * self.weights[i]\n",
    "\n",
    "            if i > 0:\n",
    "                output_error = np.dot(output_error, self.weights[i].T)\n",
    "                output_error = self._activation_backward(output_error, self.activations[i - 1],\n",
    "                                                         self.activations_cache[i])\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= lr * grad_w[i]\n",
    "            self.biases[i] -= lr * grad_b[i]\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        #cross-entropy loss for multi-class classification \n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "    def fit(self, X_train, y_train, lr, epochs, batch_size):\n",
    "        #training the network\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            for batch_start in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[batch_start:batch_start + batch_size]\n",
    "                y_batch = y_train[batch_start:batch_start + batch_size]\n",
    "\n",
    "                y_pred = self.forward(X_batch, training=True)\n",
    "                self.backward(X_batch, y_batch, lr, batch_size)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = self.forward(X_train, training=False)\n",
    "                loss = self.loss(y_train, y_pred)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('youth_smoking_drug_data_10000_rows_expanded.csv')\n",
    "\n",
    "selected_features = [\n",
    "    \"Age_Group\", \n",
    "    \"Smoking_Prevalence\",\n",
    "    \"Drug_Experimentation\",\n",
    "    \"Socioeconomic_Status\",\n",
    "    \"Parental_Supervision\",\n",
    "    \"Community_Support\",\n",
    "]\n",
    "X = data[selected_features].values\n",
    "\n",
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "for col in [\"Age_Group\", \"Socioeconomic_Status\"]:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Select and normalize features\n",
    "X = data[selected_features].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode target\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(data[[\"Access_to_Counseling\"]].values)\n",
    "\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define network parameters\n",
    "layer_sizes = [X_train.shape[1], 32, 16, 2]  \n",
    "activations = ['relu', 'relu', 'softmax']  \n",
    "dropout_rates = [0.1, 0.1, 0.0]\n",
    "\n",
    "# Initialize and train the network\n",
    "nn = FullyConnectedNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=activations,\n",
    "    dropout_rates=dropout_rates,\n",
    "    l1=0.01,\n",
    "    l2=0.01\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train, lr=0.01, epochs=200, batch_size=16)\n",
    "\n",
    "y_train_pred = nn.predict(X_train)\n",
    "train_loss = nn.loss(y_train, y_train_pred)\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "y_test_pred = nn.predict(X_test)\n",
    "test_loss = nn.loss(y_test, y_test_pred)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(np.argmax(y_test_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
