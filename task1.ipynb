{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   Year Age_Group  Gender  Smoking_Prevalence  Drug_Experimentation  \\\n",
      "0  2024     15-19    Both               18.85                 32.40   \n",
      "1  2024     10-14  Female               34.88                 41.57   \n",
      "2  2023     10-14    Both               42.00                 56.80   \n",
      "3  2024     40-49    Both               33.75                 42.90   \n",
      "4  2023     15-19    Male               47.90                 39.62   \n",
      "\n",
      "  Socioeconomic_Status  Peer_Influence School_Programs  Family_Background  \\\n",
      "0                 High               5             Yes                  1   \n",
      "1                 High               6             Yes                 10   \n",
      "2                 High               6             Yes                  2   \n",
      "3               Middle              10              No                  9   \n",
      "4                 High               1              No                  2   \n",
      "\n",
      "   Mental_Health Access_to_Counseling  Parental_Supervision  \\\n",
      "0              5                   No                     4   \n",
      "1              5                   No                     9   \n",
      "2              7                  Yes                     2   \n",
      "3              7                  Yes                     2   \n",
      "4              4                  Yes                     4   \n",
      "\n",
      "  Substance_Education  Community_Support  Media_Influence  \n",
      "0                  No                  3                1  \n",
      "1                 Yes                  9                3  \n",
      "2                  No                  5                1  \n",
      "3                  No                 10                9  \n",
      "4                  No                 10                3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('youth_smoking_drug_data_10000_rows_expanded.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Initial Shape of the Dataset: (10000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Initial Shape of the Dataset:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,x):\n",
    "        # Computed the sigmoind function\n",
    "        # The result is stored in self.output for use during the backward pass\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        return grad * self.output * (1 - self.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        # Save the input for use in the backward pass\n",
    "        self.input = x  \n",
    "        relu_output = np.maximum(0, x)\n",
    "        return np.maximum(0, x)  # Apply ReLU activation\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Create a copy of grad to avoid modifying it directly\n",
    "        grad_input = np.array(grad, copy=True)\n",
    "        # Set the gradient to 0 for all input values where x <= 0\n",
    "        grad_input[self.input <= 0] = 0\n",
    "        return grad_input  # Return the modified gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # Subtract the max value for numerical stability\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        # Normalize by dividing by the sum of exponentials\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Gradient w.r.t. the input is simply grad passed from the loss\n",
    "        # Cross-entropy loss simplifies this calculation\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        \n",
    "        if training:\n",
    "            # Generate the dropout mask (1 for active neurons, 0 for dropped neurons)\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=inputs.shape)\n",
    "            # Apply the mask and scale the active neurons\n",
    "            return inputs * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # At test time, just return the inputs unchanged\n",
    "            return inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "       \n",
    "        # Propagate gradients only through active neurons\n",
    "        return grad_output * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def update(self, param, grad, param_key=None):\n",
    "        # Abstract method to be implemented by specific optimizers.\n",
    "        # The `param` is the weight or bias being updated.\n",
    "        # The `grad` is the gradient of the loss with respect to the parameter.\n",
    "        raise NotImplementedError(\"This method should be implemented in subclasses.\")\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        # Initialize the optimizer with a specified learning rate.\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def update(self, param, grad, param_key=None):\n",
    "        # Update the parameter by subtracting the learning rate times the gradient.\n",
    "        # This is the basic gradient descent update rule.\n",
    "        return param - self.lr * grad\n",
    "\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        # Initialize the optimizer with a learning rate and a momentum factor.\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # Use a dictionary to store the velocity for each parameter.\n",
    "        self.velocity = {}\n",
    "\n",
    "    def update(self, param, grad, param_key):\n",
    "        # If this is the first time updating the parameter, initialize its velocity to zero.\n",
    "        if param_key not in self.velocity:\n",
    "            self.velocity[param_key] = np.zeros_like(param)\n",
    "\n",
    "        # Update the velocity using the momentum term and the gradient.\n",
    "        # Velocity accumulates past gradients with a damping factor given by momentum.\n",
    "        self.velocity[param_key] = self.momentum * self.velocity[param_key] - self.lr * grad\n",
    "\n",
    "        # Update the parameter by adding the velocity.\n",
    "        # This smooths updates and can help avoid local minima or oscillations.\n",
    "        return param + self.velocity[param_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "class FullyConnectedNN:\n",
    "    #initializing neural networks weights, biases and regularization\n",
    "    def __init__(self, layer_sizes, activations, dropout_rates=None, l1=0.01, l2=0.01):\n",
    "        self.layer_sizes = layer_sizes #number of neurons per layer\n",
    "        self.activations = activations #which acitvation function is used for each layer\n",
    "        self.dropout_rates = dropout_rates or [0.0] * (len(layer_sizes) - 1) #dropout rate for each layer\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "         # Initialize weights and biases\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier Initialization for weights\n",
    "            self.weights.append(\n",
    "                np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            )\n",
    "            # Initialize biases to zero\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def _activation_forward(self, x, activation):\n",
    "        #forward pass for activation functions we have coded \n",
    "        if activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'softmax':\n",
    "            exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "    def _activation_backward(self, grad, activation, output):\n",
    "        #backward pass for activation functions we have coded\n",
    "        if activation == 'relu':\n",
    "            grad[output <= 0] = 0\n",
    "            return grad\n",
    "        elif activation == 'sigmoid':\n",
    "            return grad * output * (1 - output)\n",
    "        elif activation == 'softmax':\n",
    "            return grad\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.activations_cache = [X]\n",
    "        self.z_cache = []\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.activations_cache[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_cache.append(z)\n",
    "            a = self._activation_forward(z, self.activations[i])\n",
    "            if training and self.dropout_rates[i] > 0.0:\n",
    "                dropout_mask = np.random.binomial(1, 1 - self.dropout_rates[i], size=a.shape)\n",
    "                a *= dropout_mask / (1 - self.dropout_rates[i])\n",
    "            self.activations_cache.append(a)\n",
    "        return self.activations_cache[-1]\n",
    "\n",
    "    def backward(self, X, y, optimizer, lr, batch_size):\n",
    "        m = X.shape[0]\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        output_error = self.activations_cache[-1] - y  \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(self.activations_cache[i].T, output_error) / batch_size\n",
    "            grad_b[i] = np.sum(output_error, axis=0, keepdims=True) / batch_size\n",
    "\n",
    "            grad_w[i] += self.l1 * np.sign(self.weights[i]) + self.l2 * self.weights[i]\n",
    "\n",
    "            if i > 0:\n",
    "                output_error = np.dot(output_error, self.weights[i].T)\n",
    "                output_error = self._activation_backward(output_error, self.activations[i - 1],\n",
    "                                                         self.activations_cache[i])\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = optimizer.update(self.weights[i], grad_w[i], f\"weight_{i}\")\n",
    "            self.biases[i] = optimizer.update(self.biases[i], grad_b[i], f\"bias_{i}\")\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        #cross-entropy loss for multi-class classification \n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X_train, y_train, optimizer, lr, epochs, batch_size):\n",
    "        #training the network\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            for batch_start in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[batch_start:batch_start + batch_size]\n",
    "                y_batch = y_train[batch_start:batch_start + batch_size]\n",
    "\n",
    "                y_pred = self.forward(X_batch, training=True)\n",
    "                self.backward(X_batch, y_batch, optimizer, lr, batch_size)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = self.forward(X_train, training=False)\n",
    "                loss = self.loss(y_train, y_pred)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlations with the target:\n",
      "High_Smoking            1.000000\n",
      "Smoking_Prevalence      0.864624\n",
      "Media_Influence         0.016918\n",
      "Peer_Influence          0.013378\n",
      "Mental_Health           0.009030\n",
      "Socioeconomic_Status    0.008058\n",
      "Community_Support       0.007247\n",
      "Family_Background       0.007156\n",
      "Drug_Experimentation    0.006638\n",
      "Parental_Supervision    0.006433\n",
      "Access_to_Counseling    0.001000\n",
      "Year                    0.000912\n",
      "Gender                  0.000122\n",
      "Substance_Education    -0.000600\n",
      "School_Programs        -0.004600\n",
      "Age_Group              -0.016766\n",
      "Name: High_Smoking, dtype: float64\n",
      "Selected Features by Mutual Information: Index(['Smoking_Prevalence', 'Year', 'Media_Influence', 'Access_to_Counseling',\n",
      "       'Parental_Supervision'],\n",
      "      dtype='object')\n",
      "Class distribution after SMOTE: {np.int64(0): np.int64(4000), np.int64(1): np.int64(4000)}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('youth_smoking_drug_data_10000_rows_expanded.csv')\n",
    "\n",
    "# **Create a binary target for Smoking Prevalence (High vs Low)**\n",
    "threshold = data['Smoking_Prevalence'].median()\n",
    "data['High_Smoking'] = (data['Smoking_Prevalence'] > threshold).astype(int)\n",
    "\n",
    "# Set the target as high smoking\n",
    "target = 'High_Smoking'\n",
    "y = data[target].values\n",
    "\n",
    "# Encode all categorical columns dynamically\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Calculate correlations with the new target\n",
    "print(\"Feature correlations with the target:\")\n",
    "numeric_data = data.select_dtypes(include=[np.number])  # Select numeric columns\n",
    "correlation_with_target = numeric_data.corr()[target].sort_values(ascending=False)\n",
    "print(correlation_with_target)\n",
    "\n",
    "# Feature selection using Mutual Information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi_scores = mutual_info_classif(data.drop(columns=[target]), y)\n",
    "top_features = np.argsort(mi_scores)[::-1][:5]  # Select top 5 features\n",
    "selected_features = data.drop(columns=[target]).columns[top_features]\n",
    "print(\"Selected Features by Mutual Information:\", selected_features)\n",
    "\n",
    "# Prepare the features\n",
    "X = data[selected_features].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the training set\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", dict(zip(*np.unique(y_train_smote, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.05799517064439273\n",
      "Epoch 10, Loss: 0.04286317112794572\n",
      "Epoch 20, Loss: 0.04414255502500603\n",
      "Epoch 30, Loss: 0.04189787191406467\n",
      "Epoch 40, Loss: 0.04369467827794211\n",
      "Epoch 50, Loss: 0.043009748998845236\n",
      "Epoch 60, Loss: 0.04621504647541563\n",
      "Epoch 70, Loss: 0.04304384071283049\n",
      "Epoch 80, Loss: 0.04301601431814693\n",
      "Epoch 90, Loss: 0.04270144443734852\n",
      "Epoch 100, Loss: 0.04454048875611411\n",
      "Epoch 110, Loss: 0.04355044893408096\n",
      "Epoch 120, Loss: 0.04703061228429709\n",
      "Epoch 130, Loss: 0.04314348163842661\n",
      "Epoch 140, Loss: 0.04549582854652038\n",
      "Epoch 150, Loss: 0.04215849809109616\n",
      "Epoch 160, Loss: 0.043047297712967746\n",
      "Epoch 170, Loss: 0.04313176593221869\n",
      "Epoch 180, Loss: 0.04594868761098644\n",
      "Epoch 190, Loss: 0.043224074034477235\n"
     ]
    }
   ],
   "source": [
    "# Define Neural Network Parameters\n",
    "layer_sizes = [X_train_smote.shape[1], 32, 16, 2]\n",
    "activations = ['relu', 'relu', 'sigmoid']\n",
    "dropout_rates = [0.2, 0.2, 0.0]\n",
    "\n",
    "# Initialize the Neural Network\n",
    "nn = FullyConnectedNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=activations,\n",
    "    dropout_rates=dropout_rates,\n",
    "    l1=0.005,\n",
    "    l2=0.005\n",
    ")\n",
    "\n",
    "# Train the Neural Network\n",
    "optimizer = SGDMomentum(learning_rate=0.005, momentum=0.9)\n",
    "nn.fit(X_train_smote, np.eye(2)[y_train_smote], optimizer=optimizer, lr=0.005, epochs=200, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.04483601106885576\n",
      "Test Loss: 0.04899424745284426\n",
      "Test Accuracy: 0.9935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1000\n",
      "           1       1.00      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "y_train_pred = nn.predict(X_train_smote)\n",
    "train_loss = nn.loss(np.eye(2)[y_train_smote], y_train_pred)\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_test_pred = nn.predict(X_test)\n",
    "test_loss = nn.loss(np.eye(2)[y_test], y_test_pred)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Calculate Test Accuracy\n",
    "accuracy = np.mean(np.argmax(y_test_pred, axis=1) == y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, np.argmax(y_test_pred, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
